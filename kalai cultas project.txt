"""
Advanced Time Series Forecasting with LSTM + Self-Attention

Author: Kalai Arasan
Description:
Multivariate time series forecasting using LSTM enhanced
with a self-attention mechanism. Compared against ARIMA
baseline using rolling-origin evaluation.
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
import random

# ----------------------------
# Reproducibility
# ----------------------------
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

# ----------------------------
# Synthetic Dataset Generator
# ----------------------------
def generate_synthetic_data(n_steps=1500):
    """
    Generate multivariate time series with trend, seasonality and noise.

    Parameters
    ----------
    n_steps : int
        Number of time steps.

    Returns
    -------
    np.ndarray
        Multivariate time series data.
    """
    t = np.arange(n_steps)
    series_1 = 0.05 * t + np.sin(0.02 * t) + np.random.normal(0, 0.3, n_steps)
    series_2 = np.cos(0.015 * t) + np.random.normal(0, 0.2, n_steps)
    series_3 = np.sin(0.01 * t) + 0.03 * t + np.random.normal(0, 0.25, n_steps)
    return np.vstack([series_1, series_2, series_3]).T


# ----------------------------
# Windowing Function
# ----------------------------
def create_windows(data, window_size=30, horizon=1):
    """
    Convert time series into supervised learning format.

    Parameters
    ----------
    data : np.ndarray
        Input multivariate data.
    window_size : int
        Number of past time steps.
    horizon : int
        Forecast horizon.

    Returns
    -------
    tuple
        X, y arrays.
    """
    X, y = [], []
    for i in range(len(data) - window_size - horizon):
        X.append(data[i:i + window_size])
        y.append(data[i + window_size:i + window_size + horizon, 0])
    return np.array(X), np.array(y)


# ----------------------------
# Attention Layer
# ----------------------------
class SelfAttention(layers.Layer):
    """
    Self-attention mechanism for sequence data.
    """

    def __init__(self):
        super(SelfAttention, self).__init__()

    def call(self, inputs):
        scores = tf.matmul(inputs, inputs, transpose_b=True)
        weights = tf.nn.softmax(scores, axis=-1)
        attended = tf.matmul(weights, inputs)
        return attended, weights


# ----------------------------
# Model Builder
# ----------------------------
def build_attention_lstm(input_shape):
    """
    Build LSTM model with self-attention.

    Parameters
    ----------
    input_shape : tuple
        Shape of input data.

    Returns
    -------
    tf.keras.Model
    """
    inputs = layers.Input(shape=input_shape)
    x = layers.LSTM(64, return_sequences=True)(inputs)
    attn_out, attn_weights = SelfAttention()(x)
    x = layers.GlobalAveragePooling1D()(attn_out)
    outputs = layers.Dense(1)(x)

    model = models.Model(inputs, outputs)
    model.compile(
        optimizer='adam',
        loss='mse'
    )
    return model


# ----------------------------
# Rolling Origin Evaluation
# ----------------------------
def rolling_forecast(model, X, y, splits=3):
    """
    Perform rolling-origin cross-validation.

    Parameters
    ----------
    model : tf.keras.Model
    X : np.ndarray
    y : np.ndarray
    splits : int

    Returns
    -------
    float
        Average RMSE.
    """
    split_size = len(X) // splits
    rmses = []

    for i in range(splits):
        end = (i + 1) * split_size
        X_train, X_test = X[:end], X[end:end + split_size]
        y_train, y_test = y[:end], y[end:end + split_size]

        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
        preds = model.predict(X_test, verbose=0)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        rmses.append(rmse)

    return np.mean(rmses)


# ----------------------------
# Baseline ARIMA
# ----------------------------
def arima_baseline(series, order=(5, 1, 0)):
    """
    ARIMA baseline model.

    Parameters
    ----------
    series : np.ndarray
        Univariate time series.

    Returns
    -------
    float
        RMSE score.
    """
    train_size = int(len(series) * 0.8)
    train, test = series[:train_size], series[train_size:]

    model = ARIMA(train, order=order)
    fitted = model.fit()
    forecast = fitted.forecast(steps=len(test))
    rmse = np.sqrt(mean_squared_error(test, forecast))
    return rmse


# ----------------------------
# Main Execution
# ----------------------------
if __name__ == "__main__":
    data = generate_synthetic_data()
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data)

    X, y = create_windows(data_scaled)

    model = build_attention_lstm(X.shape[1:])
    lstm_rmse = rolling_forecast(model, X, y)

    arima_rmse = arima_baseline(data[:, 0])

    print("LSTM + Attention RMSE:", lstm_rmse)
    print("ARIMA Baseline RMSE:", arima_rmse)